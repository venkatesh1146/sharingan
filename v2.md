# Market News Summarization System - Technical Design Summary

## Tech Stack
- **Database**: MongoDB 7.0+
- **Task Queue**: Celery with Redis broker
- **Cache**: Redis
- **API**: FastAPI (Python 3.11+)
- **AI**:  API for summarization & analysis

---

## MongoDB Schema Design

### Database: `market_intelligence`

### Collection 1: `market_snapshots`
**Purpose**: Aggregated market views generated periodically (every 15-30 min)

**Document Structure**:
```json
{
  "_id": ObjectId("..."),
  "snapshot_id": "snapshot_2026-01-30_15:21_post",
  "generated_at": ISODate("2026-01-30T15:21:45.524Z"),
  "request_id": "198b82a6-daf3-4d0e-8cba-fd0ddf681d2b",
  "market_phase": "post",
  
  "market_outlook": {
    "sentiment": "neutral",
    "confidence": 0.7,
    "reasoning": "NIFTY 50 is relatively flat at -0.39%...",
    "nifty_change_percent": -0.39,
    "key_drivers": ["Broad-based movement"]
  },
  
  "indices_data": [
    {
      "ticker": "NIFTY",
      "name": "Nifty",
      "country": "India",
      "current_price": 25320.65,
      "change_percent": -0.39,
      "change_absolute": -98.25,
      "previous_close": 25418.9,
      "intraday_high": 25320.65,
      "intraday_low": 25320.65,
      "volume": 0,
      "timestamp": ISODate("2026-01-30T00:00:00Z")
    }
  ],
  
  "market_summary": [
    {
      "text": "Economic outlook shows mixed sentiment...",
      "supporting_news_ids": ["1669331"],
      "confidence": 0.6,
      "sentiment": "neutral"
    }
  ],
  
  "executive_summary": "Markets trading flat with NIFTY at -0.4%.",
  "trending_now": null,
  "themed_news": [],
  "all_news_ids": ["1669331", "1669330", "1669328"],
  
  "watchlist_impacted": [],
  "watchlist_alerts": [],
  "portfolio_impact_summary": "No portfolio holdings to analyze",
  "portfolio_sentiment": "neutral",
  
  "degraded_mode": false,
  "warnings": [],
  "expires_at": ISODate("2026-01-30T15:36:45Z"),
  "created_at": ISODate("2026-01-30T15:21:45Z"),
  "version": "1.0"
}
```

**Field Types**:
- `_id`: ObjectId (auto-generated)
- `snapshot_id`: String (unique identifier)
- `generated_at`: ISODate
- `request_id`: String (UUID)
- `market_phase`: String (enum: "pre", "live", "post")
- `market_outlook`: Object
  - `sentiment`: String (enum: "bullish", "bearish", "neutral")
  - `confidence`: Number (0.0-1.0)
  - `reasoning`: String
  - `nifty_change_percent`: Number (decimal)
  - `key_drivers`: Array of Strings
- `indices_data`: Array of Objects (embedded)
- `market_summary`: Array of Objects
- `all_news_ids`: Array of Strings
- `expires_at`: ISODate (for TTL)

**Indexes**:
```javascript
db.market_snapshots.createIndex(
  { "generated_at": -1 }, 
  { name: "idx_generated_at" }
)

db.market_snapshots.createIndex(
  { "market_phase": 1, "generated_at": -1 }, 
  { name: "idx_phase_time" }
)

db.market_snapshots.createIndex(
  { "snapshot_id": 1 }, 
  { unique: true, name: "idx_snapshot_id" }
)

db.market_snapshots.createIndex(
  { "expires_at": 1 }, 
  { expireAfterSeconds: 0, name: "idx_ttl_expire" }
)
```

---

### Collection 2: `news_articles`
**Purpose**: Normalized storage of individual news articles with AI analysis

**Document Structure**:
```json
{
  "_id": ObjectId("..."),
  "news_id": "1669331",
  "headline": "Fiscal deficit at 54.5% of FY26 budget estimates",
  "summary": "India's fiscal deficit narrowed 6.4% YoY to ₹8.56 lakh crore...",
  "full_text": "Complete article text if available",
  
  "source": "Economy - Reports",
  "source_url": "https://example.com/news/...",
  "published_at": ISODate("2026-01-30T18:07:00Z"),
  "fetched_at": ISODate("2026-01-30T18:08:15Z"),
  
  "sentiment": "neutral",
  "is_breaking": true,
  "category": "economy",
  "subcategory": "fiscal_policy",
  
  "mentioned_stocks": [],
  "mentioned_sectors": ["Economy", "Macro", "Policy"],
  "mentioned_companies": [],
  "mentioned_people": [],
  "mentioned_locations": ["India"],
  
  "impacted_stocks": [],
  "sector_impacts": {},
  "causal_chain": "No direct stock impact identified",
  
  "content_hash": "sha256_abc123...",
  "url_hash": "sha256_xyz789...",
  "duplicate_of": null,
  
  "keywords": ["fiscal", "deficit", "budget"],
  "embeddings": [0.1, 0.2, 0.3, ...],
  
  "processed": true,
  "analyzed": true,
  "included_in_snapshots": ["snapshot_2026-01-30_post"],
  
  "created_at": ISODate("2026-01-30T18:08:15Z"),
  "updated_at": ISODate("2026-01-30T18:08:15Z")
}
```

**Field Types**:
- `_id`: ObjectId
- `news_id`: String (unique, external ID)
- `headline`: String
- `summary`: String (AI-generated)
- `full_text`: String (optional)
- `source`: String
- `source_url`: String
- `published_at`: ISODate
- `fetched_at`: ISODate
- `sentiment`: String (enum: "bullish", "bearish", "neutral")
- `is_breaking`: Boolean
- `category`: String
- `subcategory`: String
- `mentioned_stocks`: Array of Strings
- `mentioned_sectors`: Array of Strings
- `mentioned_companies`: Array of Strings
- `mentioned_people`: Array of Strings
- `mentioned_locations`: Array of Strings
- `impacted_stocks`: Array of Objects
- `sector_impacts`: Object
- `causal_chain`: String
- `content_hash`: String (SHA256)
- `url_hash`: String (SHA256)
- `duplicate_of`: String (news_id reference, nullable)
- `keywords`: Array of Strings
- `embeddings`: Array of Numbers (float32, 1536 dimensions for OpenAI/Existing AI framework embeddings)
- `processed`: Boolean
- `analyzed`: Boolean
- `included_in_snapshots`: Array of Strings
- `created_at`: ISODate
- `updated_at`: ISODate

**Indexes**:
```javascript
db.news_articles.createIndex(
  { "news_id": 1 }, 
  { unique: true, name: "idx_news_id" }
)

db.news_articles.createIndex(
  { "published_at": -1 }, 
  { name: "idx_published_at" }
)

db.news_articles.createIndex(
  { "content_hash": 1 }, 
  { unique: true, sparse: true, name: "idx_content_hash" }
)

db.news_articles.createIndex(
  { "url_hash": 1 }, 
  { name: "idx_url_hash" }
)

db.news_articles.createIndex(
  { "sentiment": 1, "published_at": -1 }, 
  { name: "idx_sentiment_time" }
)

db.news_articles.createIndex(
  { "mentioned_stocks": 1 }, 
  { name: "idx_mentioned_stocks" }
)

db.news_articles.createIndex(
  { "mentioned_sectors": 1 }, 
  { name: "idx_mentioned_sectors" }
)

db.news_articles.createIndex(
  { "is_breaking": 1, "published_at": -1 }, 
  { name: "idx_breaking_news" }
)

db.news_articles.createIndex(
  { "processed": 1, "analyzed": 1 }, 
  { name: "idx_processing_status" }
)

// For semantic search (requires MongoDB Atlas Search or vector search plugin)
db.news_articles.createIndex(
  { "embeddings": "vector" },
  { 
    name: "idx_embeddings_vector",
    type: "vectorSearch",
    numDimensions: 1536,
    similarity: "cosine"
  }
)
```

---

### Collection 3: `indices_timeseries` (Optional - for historical tracking)
**Purpose**: Time-series data for indices (if you want separate historical storage)

**Document Structure**:
```json
{
  "_id": ObjectId("..."),
  "ticker": "NIFTY",
  "timestamp": ISODate("2026-01-30T15:21:45Z"),
  "data": {
    "name": "Nifty",
    "country": "India",
    "current_price": 25320.65,
    "change_percent": -0.39,
    "change_absolute": -98.25,
    "previous_close": 25418.9,
    "intraday_high": 25320.65,
    "intraday_low": 25320.65,
    "volume": 0
  },
  "created_at": ISODate("2026-01-30T15:21:45Z")
}
```

**Field Types**:
- `_id`: ObjectId
- `ticker`: String
- `timestamp`: ISODate
- `data`: Object (flexible structure)
- `created_at`: ISODate

**Indexes**:
```javascript
db.indices_timeseries.createIndex(
  { "ticker": 1, "timestamp": -1 }, 
  { name: "idx_ticker_time" }
)

db.indices_timeseries.createIndex(
  { "timestamp": 1 }, 
  { name: "idx_timestamp" }
)

// TTL to auto-delete old data (e.g., after 90 days)
db.indices_timeseries.createIndex(
  { "created_at": 1 }, 
  { expireAfterSeconds: 7776000, name: "idx_ttl_90days" }
)
```

---

## Celery Task Architecture

### Celery Configuration
```python
# celery_config.py
from celery import Celery
from celery.schedules import crontab

app = Celery(
    'market_news',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Kolkata',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,  # 5 minutes
    task_soft_time_limit=240,  # 4 minutes
    worker_prefetch_multiplier=4,
    worker_max_tasks_per_child=1000,
)

# Periodic Tasks
app.conf.beat_schedule = {
    'fetch-news-every-15-min': {
        'task': 'tasks.fetch_and_process_news',
        'schedule': 900.0,  # 15 minutes
    },
    'generate-market-snapshot-every-30-min': {
        'task': 'tasks.generate_market_snapshot',
        'schedule': 1800.0,  # 30 minutes
        'args': ('live',)
    },
    'cleanup-old-snapshots-daily': {
        'task': 'tasks.cleanup_old_data',
        'schedule': crontab(hour=2, minute=0),  # 2 AM daily
    },
}
```

### Celery Tasks

#### Task 1: `fetch_and_process_news`
```python
@app.task(name='tasks.fetch_and_process_news', bind=True, max_retries=3)
def fetch_and_process_news(self):
    """
    Fetch news from sources and process them.
    Runs every 15 minutes.
    """
    # 1. Fetch from news APIs (NewsAPI, RSS feeds, etc.)
    # 2. Deduplicate based on content_hash/url_hash
    # 3. For each new article:
    #    - Generate summary (AI)
    #    - Analyze sentiment (AI)
    #    - Extract entities (stocks, sectors, people)
    #    - Generate embeddings
    #    - Store in news_articles collection
    # 4. Return count of new articles processed
```

#### Task 2: `process_single_article`
```python
@app.task(name='tasks.process_single_article', bind=True, max_retries=2)
def process_single_article(self, article_data):
    """
    Process a single news article.
    Called by fetch_and_process_news for parallel processing.
    """
    # 1. Check for duplicates
    # 2. AI summarization
    # 3. Sentiment analysis
    # 4. Entity extraction
    # 5. Generate embeddings
    # 6. Store in MongoDB
    # 7. Return news_id
```

#### Task 3: `generate_market_snapshot`
```python
@app.task(name='tasks.generate_market_snapshot', bind=True)
def generate_market_snapshot(self, phase='post'):
    """
    Generate aggregated market snapshot.
    Runs every 30 minutes or on-demand.
    
    Args:
        phase: 'pre', 'live', or 'post'
    """
    # 1. Fetch recent news (last 4 hours)
    # 2. Fetch latest indices data
    # 3. AI analysis:
    #    - Market outlook
    #    - Market summaries (grouped by theme)
    #    - Executive summary
    # 4. Build snapshot document
    # 5. Store in market_snapshots
    # 6. Return snapshot_id
```

#### Task 4: `fetch_indices_data`
```python
@app.task(name='tasks.fetch_indices_data', bind=True)
def fetch_indices_data(self):
    """
    Fetch live indices data from NSE/BSE APIs.
    Runs every 5 minutes during market hours.
    """
    # 1. Call NSE/BSE APIs
    # 2. Transform data
    # 3. Store in indices_timeseries (optional)
    # 4. Cache in Redis for quick access
    # 5. Return indices data
```

#### Task 5: `cleanup_old_data`
```python
@app.task(name='tasks.cleanup_old_data', bind=True)
def cleanup_old_data(self):
    """
    Cleanup old snapshots and news.
    Runs daily at 2 AM.
    """
    # 1. Delete expired snapshots (TTL handles this, but backup cleanup)
    # 2. Archive old news (>90 days) to cold storage
    # 3. Clean up Redis cache
    # 4. Log cleanup stats
```

#### Task 6: `generate_themed_news`
```python
@app.task(name='tasks.generate_themed_news', bind=True)
def generate_themed_news(self, theme_keywords):
    """
    Group related news into themes.
    Called periodically or on-demand.
    """
    # 1. Semantic search using embeddings
    # 2. Cluster similar articles
    # 3. Generate theme summary
    # 4. Store in themed_news_groups
```

---

## Redis Caching Strategy

### Cache Keys Structure
```
# Market snapshots (15-minute cache)
cache:snapshot:{phase}:latest -> JSON string

# Indices data (5-minute cache)
cache:indices:latest -> JSON string

# News article (1-hour cache)
cache:news:{news_id} -> JSON string

# API response cache (10-minute cache)
cache:api:market-summary:{phase}:{timestamp} -> JSON string
```

### Cache Configuration
```python
import redis
from datetime import timedelta

redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=2,  # Separate DB for cache
    decode_responses=True
)

CACHE_EXPIRY = {
    'snapshot': 900,      # 15 minutes
    'indices': 300,       # 5 minutes
    'news': 3600,         # 1 hour
    'api_response': 600   # 10 minutes
}
```

---

## Data Flow Architecture

### 1. News Ingestion Pipeline
```
News Sources (API/RSS)
    ↓
[Celery Task: fetch_and_process_news] (every 15 min)
    ↓
[Parallel Tasks: process_single_article] (for each article)
    ↓
AI Processing (API):
    - Summarization
    - Sentiment Analysis
    - Entity Extraction
    - Embedding Generation
    ↓
Deduplication Check (content_hash)
    ↓
MongoDB: news_articles collection
```

### 2. Market Snapshot Generation
```
[Celery Task: generate_market_snapshot] (every 30 min)
    ↓
Query MongoDB: Recent news (last 4 hours)
    ↓
Fetch Indices: From cache or API
    ↓
AI Analysis (Existing AI framework API):
    - Market outlook
    - Market summaries
    - Executive summary
    ↓
Build Snapshot Document
    ↓
MongoDB: market_snapshots collection
    ↓
Redis Cache: cache:snapshot:{phase}:latest
```

### 3. API Request Flow
```
API Request: GET /api/market-summary?phase=post
    ↓
Check Redis Cache: cache:api:market-summary:post:{timestamp}
    ↓ (cache miss)
Query MongoDB: market_snapshots (latest by phase)
    ↓
If expired/missing: Trigger generate_market_snapshot task
    ↓
Hydrate News Details: Query news_articles by news_ids
    ↓
Build Response JSON (already in correct format)
    ↓
Cache in Redis (10 min TTL)
    ↓
Return Response
```

---

## AI Processing Details

### Existing AI framework API Usage

#### 1. News Summarization
```python
async def generate_summary(article_text: str) -> str:
    """Generate concise summary of news article"""
    prompt = f"""Summarize this financial news article in 2-3 sentences.
    Focus on key facts, numbers, and market impact.
    
    Article: {article_text}
    
    Summary:"""
    # Call Existing AI framework API
```

#### 2. Sentiment Analysis
```python
async def analyze_sentiment(summary: str) -> str:
    """Analyze market sentiment: bullish, bearish, or neutral"""
    prompt = f"""Analyze the market sentiment of this news.
    Return only: bullish, bearish, or neutral.
    
    News: {summary}
    
    Sentiment:"""
    # Call Existing AI framework API
```

#### 3. Market Outlook Generation
```python
async def analyze_market_outlook(news_list, indices_data) -> dict:
    """Generate comprehensive market outlook"""
    prompt = f"""Based on recent news and market indices, provide:
    1. Overall sentiment (bullish/bearish/neutral)
    2. Confidence score (0.0-1.0)
    3. Reasoning (2-3 sentences)
    4. Key market drivers (list)
    
    News: {news_list}
    Indices: {indices_data}
    
    Return as JSON:"""
    # Call Existing AI framework API with structured output
```

#### 4. Entity Extraction
```python
async def extract_entities(text: str) -> dict:
    """Extract stocks, sectors, companies, people mentioned"""
    prompt = f"""Extract entities from this financial news:
    - Stock tickers/names
    - Sectors (Economy, IT, Banking, etc.)
    - Company names
    - Key people mentioned
    
    Text: {text}
    
    Return as JSON:"""
    # Call Existing AI framework API
```

---

## API Endpoint Specification

### Endpoint: `GET /api/market-summary`

**Query Parameters**:
- `phase` (optional): "pre" | "live" | "post" (default: "post")
- `include_news_details` (optional): boolean (default: true)
- `news_limit` (optional): integer (default: 10, max: 50)

**Response Format**: (matches your provided JSON exactly)
```json
{
  "generated_at": "ISO8601 timestamp",
  "request_id": "UUID",
  "market_phase": "post",
  "market_outlook": {...},
  "indices_data": [...],
  "market_summary": [...],
  "executive_summary": "string",
  "trending_now": null,
  "themed_news": [],
  "all_news": [...],
  "watchlist_impacted": [],
  "watchlist_alerts": [],
  "portfolio_impact_summary": "string",
  "portfolio_sentiment": "neutral",
  "degraded_mode": false,
  "warnings": []
}
```

**Response Time Target**: < 200ms (with caching)

---

## Deployment Notes

### MongoDB Configuration
```bash
# Enable replica set for transactions (if needed)
mongod --replSet rs0

# In mongo shell:
rs.initiate()

# Create database and collections
use market_intelligence
db.createCollection("market_snapshots")
db.createCollection("news_articles")
db.createCollection("indices_timeseries")
```

### Celery Workers
```bash
# Start Celery worker
celery -A tasks worker --loglevel=info --concurrency=4

# Start Celery beat (scheduler)
celery -A tasks beat --loglevel=info

# Monitor with Flower
celery -A tasks flower --port=5555
```

### Redis Configuration
```bash
# Start Redis
redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru
```

---

## Performance Optimization

### 1. Database
- Use projection to fetch only required fields
- Compound indexes for common query patterns
- Use `$in` operator efficiently for news hydration
- Enable oplog for change streams (if real-time needed)

### 2. Caching
- Cache market snapshots for 15 minutes
- Cache individual news for 1 hour
- Use Redis pipeline for bulk operations
- Implement cache warming for popular queries

### 3. Celery
- Use task prioritization (high priority for snapshot generation)
- Implement task result expiry
- Use Celery chains for dependent tasks
- Monitor task queue depth

### 4. AI API
- Batch similar requests when possible
- Implement retry logic with exponential backoff
- Cache AI responses for identical inputs
- Use streaming for long-running tasks

---

## Monitoring & Alerts

### Key Metrics to Track
1. News fetch success rate
2. AI processing latency
3. Snapshot generation time
4. API response time (p50, p95, p99)
5. Cache hit rate
6. Celery queue depth
7. MongoDB query performance
8. Redis memory usage

### Alert Conditions
- News fetch failures > 3 consecutive attempts
- Snapshot generation time > 60 seconds
- API response time > 1 second
- Celery queue depth > 100 tasks
- MongoDB connection errors
- Redis memory > 80%

---

## Summary Checklist for Implementation

- [ ] Set up MongoDB with all collections and indexes
- [ ] Configure Redis for caching and Celery broker
- [ ] Implement Celery tasks (6 main tasks)
- [ ] Set up Existing AI framework API integration for AI processing
- [ ] Build FastAPI endpoints
- [ ] Implement caching layer
- [ ] Set up monitoring (Prometheus + Grafana)
- [ ] Configure logging (structured logs)
- [ ] Write unit tests for critical paths
- [ ] Deploy workers and API service
- [ ] Set up alerting (PagerDuty/Slack)
- [ ] Performance testing (load testing)
- [ ] Documentation (API docs, runbooks)
